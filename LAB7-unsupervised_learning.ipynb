{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Unsupervised Learning\n",
        "- Clustering\n",
        "    - K-Means Clustering\n",
        "    - Hierarchical Clustering\n",
        "- Performance Evaluation Techniques\n",
        "    - Silhouette Score\n",
        "    - Davies-Boulding Index - DBI\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "### Clustering\n",
        "\n",
        "- When you need to group similar data points without labeled outputs.\n",
        "- Useful for exploratory analysis, customer segmentation, anomaly detection, etc.\n",
        "- Works well when patterns or natural groupings exist in the data.\n",
        "\n",
        "<table border=\"1\"> <tr> <th>Pros</th> <th>Cons</th> </tr> <tr> <td>Unsupervised → no need for labeled data.</td> <td>Results may vary with initialization (e.g., K-Means clustering).</td> </tr> <tr> <td>Helps discover hidden structures/patterns in data.</td> <td>Choosing the right number of clusters can be challenging.</td> </tr> <tr> <td>Applicable in many domains → marketing, biology, image analysis, anomaly detection.</td> <td>Cluster boundaries may be unclear when data overlaps.</td> </tr> <tr> <td>Scalable methods exist for large datasets.</td> <td>Sensitive to noise and outliers → can distort clusters.</td> </tr> <tr> <td>Can be combined with dimensionality reduction for better insights.</td> <td>Interpretability may be limited → clusters don't always have clear meaning.</td> </tr> </table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### K-Means Clustering\n",
        "- Works best when clusters are roughly equally sized, and well separated.\n",
        "- Simple, fast, and easy to implement and works well as a preprocessing step for other algorithms.\n",
        "- However, requires the number of clusters (k) to be specified in advance.\n",
        "- Sensitive to initialization (can converge to local minima).\n",
        "- Affected by outliers and noise, which can distort centroids.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# K-Means Clustering with Elbow Method\n",
        "# IMPORTANT: Set environment variables BEFORE importing sklearn/joblib to avoid warnings\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "# Fix for Windows MKL memory leak warning in KMeans\n",
        "os.environ['OMP_NUM_THREADS'] = '4'  # Set to a reasonable value (4-6 works well)\n",
        "# Fix for joblib/loky core detection warning on Windows\n",
        "os.environ['LOKY_MAX_CPU_COUNT'] = '4'  # Set to number of logical cores or desired value\n",
        "\n",
        "# Suppress specific warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning)  # Suppress joblib and sklearn warnings\n",
        "warnings.filterwarnings('ignore', message='KMeans is known to have a memory leak')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Load and prepare dataset\n",
        "# -----------------------------\n",
        "# Load car price dataset\n",
        "df = pd.read_csv('primary_features_boolean_converted_final.csv')\n",
        "df = df.dropna(subset=['price(Georgian Lari)'])  # Remove rows with missing target\n",
        "\n",
        "# Feature engineering - create meaningful features for clustering\n",
        "df['vehicle_age'] = 2024 - df['product_year']  # Age affects value and condition\n",
        "df['luxury_score'] = df[['engine_volume', 'cylinders', 'airbags']].sum(axis=1)  # Luxury indicators\n",
        "df['safety_score'] = df[['airbags', 'ABS', 'ESP', 'Central Locking', 'Alarm System']].sum(axis=1)  # Safety features\n",
        "\n",
        "# Select numerical features for clustering\n",
        "numerical_features = ['vehicle_age', 'luxury_score', 'safety_score', 'mileage', 'engine_volume', 'price(Georgian Lari)']\n",
        "X = df[numerical_features].fillna(0)  # Fill missing values with 0\n",
        "\n",
        "# Sample data for faster computation (especially for clustering)\n",
        "# Using sample for faster computation while maintaining representativeness\n",
        "SAMPLE_SIZE = 1500  # Adjust based on your dataset size\n",
        "if len(X) > SAMPLE_SIZE:\n",
        "    X_sample = X.sample(n=min(SAMPLE_SIZE, len(X)), random_state=42).reset_index(drop=True)\n",
        "    print(f\"Sampling {len(X_sample)} samples from {len(X)} total samples for faster computation\")\n",
        "else:\n",
        "    X_sample = X.reset_index(drop=True) if hasattr(X, 'reset_index') else X\n",
        "    print(f\"Using full dataset: {len(X_sample)} samples\")\n",
        "\n",
        "# Standardize features (important for K-Means)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_sample)\n",
        "\n",
        "print(f\"Dataset shape: {X_scaled.shape}\")\n",
        "print(f\"Features: {numerical_features}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 2. Elbow Method to find optimal k - The Elbow Method is one of the most popular techniques to determine the optimal number of clusters (k) in K-Means clustering.\n",
        "# -----------------------------\n",
        "wcss = []  # Within-Cluster Sum of Squares. Also called Inertia in scikit-learn.\n",
        "\n",
        "for k in range(1, 15):              # Try k = 1 to 15. We test multiple values because the best k is not known in advance.\n",
        "    kmeans = KMeans(\n",
        "        n_clusters=k, \n",
        "        init=\"k-means++\",           # centroid initialization method that speeds up convergence and avoids poor clustering results. [random, forgy, random_partition, etc.]\n",
        "        random_state=42\n",
        "    )\n",
        "    kmeans.fit(X_scaled)                   # Fits the KMeans model on the dataset X (learns the cluster centroids)\n",
        "    wcss.append(kmeans.inertia_)    # Inertia = WCSS. scikit-learn's attribute that stores the final WCSS for the fitted clustering model.\n",
        "\n",
        "# Plot the Elbow Curve\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, 15), wcss, marker=\"o\", linestyle=\"--\")\n",
        "plt.title(\"Elbow Method for Optimal k\")\n",
        "plt.xlabel(\"Number of Clusters (k)\")\n",
        "plt.ylabel(\"WCSS\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 3. Fit KMeans with optimal k\n",
        "# -----------------------------\n",
        "optimal_k = 4  # From elbow curve (pick after visual inspection) - adjust based on your elbow plot\n",
        "kmeans = KMeans(n_clusters=optimal_k, init=\"k-means++\", random_state=42)\n",
        "y_kmeans = kmeans.fit_predict(X_scaled) # fit_predict(X) both trains the model and tells you which cluster each point belongs to. y_kmeans is basically the map of data points to clusters.\n",
        "\n",
        "# -----------------------------\n",
        "# Visualize Clusters (using first 2 features for 2D visualization)\n",
        "# -----------------------------\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y_kmeans, s=30, cmap=\"viridis\")\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c=\"red\", marker=\"X\", label=\"Centroids\")\n",
        "plt.title(f\"K-Means Clustering (k={optimal_k})\")\n",
        "plt.xlabel(f\"Feature 0: {numerical_features[0]}\")\n",
        "plt.ylabel(f\"Feature 1: {numerical_features[1]}\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Print cluster information\n",
        "print(f\"\\nCluster sizes:\")\n",
        "for i in range(optimal_k):\n",
        "    cluster_size = np.sum(y_kmeans == i)\n",
        "    print(f\"Cluster {i}: {cluster_size} samples ({cluster_size/len(y_kmeans)*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Hierarchical Clustering\n",
        "- Does not require the number of clusters to be specified in advance.\n",
        "- Suitable for small to medium-sized datasets; becomes computationally expensive for very large datasets.\n",
        "- Can capture nested clusters and works well when clusters are not necessarily spherical.\n",
        "- However, sensitive to noise and outliers, which can create misleading merges.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hierarchical Clustering Example\n",
        "# Set environment variables to avoid warnings (if not already set)\n",
        "import os\n",
        "os.environ.setdefault('OMP_NUM_THREADS', '4')\n",
        "os.environ.setdefault('LOKY_MAX_CPU_COUNT', '4')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "warnings.filterwarnings('ignore', message='KMeans is known to have a memory leak')\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Prepare dataset for hierarchical clustering\n",
        "# -----------------------------\n",
        "# Use a smaller sample for hierarchical clustering as it's computationally expensive\n",
        "# Note: Make sure to run the K-Means section first to define X_sample\n",
        "HIERARCHICAL_SAMPLE_SIZE = 500  # Smaller sample for faster computation\n",
        "\n",
        "# If X_sample is not defined, reload and prepare data\n",
        "if 'X_sample' not in globals():\n",
        "    # Reload data if needed\n",
        "    df_hc = pd.read_csv('primary_features_boolean_converted_final.csv')\n",
        "    df_hc = df_hc.dropna(subset=['price(Georgian Lari)'])\n",
        "    df_hc['vehicle_age'] = 2024 - df_hc['product_year']\n",
        "    df_hc['luxury_score'] = df_hc[['engine_volume', 'cylinders', 'airbags']].sum(axis=1)\n",
        "    df_hc['safety_score'] = df_hc[['airbags', 'ABS', 'ESP', 'Central Locking', 'Alarm System']].sum(axis=1)\n",
        "    numerical_features_hc = ['vehicle_age', 'luxury_score', 'safety_score', 'mileage', 'engine_volume', 'price(Georgian Lari)']\n",
        "    X_hc_full = df_hc[numerical_features_hc].fillna(0)\n",
        "    X_sample_hc = X_hc_full.sample(n=min(HIERARCHICAL_SAMPLE_SIZE, len(X_hc_full)), random_state=42)\n",
        "    X_hierarchical = X_sample_hc\n",
        "    print(f\"Sampling {len(X_hierarchical)} samples for hierarchical clustering\")\n",
        "else:\n",
        "    if len(X_sample) > HIERARCHICAL_SAMPLE_SIZE:\n",
        "        X_hierarchical = X_sample.sample(n=min(HIERARCHICAL_SAMPLE_SIZE, len(X_sample)), random_state=42)\n",
        "        print(f\"Sampling {len(X_hierarchical)} samples for hierarchical clustering\")\n",
        "    else:\n",
        "        X_hierarchical = X_sample\n",
        "        print(f\"Using {len(X_hierarchical)} samples for hierarchical clustering\")\n",
        "\n",
        "# Standardize features\n",
        "scaler_hc = StandardScaler()\n",
        "X_hierarchical_scaled = scaler_hc.fit_transform(X_hierarchical)\n",
        "\n",
        "print(f\"Dataset shape for hierarchical clustering: {X_hierarchical_scaled.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 2. Plot Dendrogram - plots a tree diagram (dendrogram) that shows the hierarchy of clusters.\n",
        "# -----------------------------\n",
        "plt.figure(figsize=(16, 8))\n",
        "linked = linkage(X_hierarchical_scaled, method=\"ward\")  # calculates how to merge points/clusters step by step\n",
        "dendrogram(linked, truncate_mode=\"lastp\", p=10, show_contracted=True)  # 'lastp' shows the last p clusters\n",
        "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
        "plt.xlabel(\"Data Points\")\n",
        "plt.ylabel(\"Distance\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What it shows:\n",
        "- X-axis: Data points (or clusters).\n",
        "- Y-axis: Distance at which clusters are merged.\n",
        "- Branches: Each branch represents a cluster merge.\n",
        "- Height: Higher merge means clusters are more dissimilar.\n",
        "\n",
        "Use:\n",
        "- By cutting the dendrogram at a certain height, you can decide the number of clusters without specifying it in advance.\n",
        "- Taller vertical lines indicate merges of very different clusters; short lines indicate merges of very similar clusters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 3. Apply Agglomerative Clustering \n",
        "# -----------------------------\n",
        "hc = AgglomerativeClustering( \n",
        "    n_clusters=6,  # Adjust based on dendrogram inspection\n",
        "    metric=\"euclidean\", \n",
        "    linkage=\"ward\"\n",
        ")\n",
        "y_hc = hc.fit_predict(X_hierarchical_scaled)\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Visualize Final Clusters\n",
        "# -----------------------------\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.scatter(X_hierarchical_scaled[:, 0], X_hierarchical_scaled[:, 1], c=y_hc, cmap=\"viridis\", s=30)\n",
        "plt.title(\"Agglomerative Hierarchical Clustering\")\n",
        "# Use feature names if available, otherwise use generic labels\n",
        "try:\n",
        "    plt.xlabel(f\"Feature 0: {numerical_features[0]}\")\n",
        "    plt.ylabel(f\"Feature 1: {numerical_features[1]}\")\n",
        "except:\n",
        "    plt.xlabel(\"Feature 0\")\n",
        "    plt.ylabel(\"Feature 1\")\n",
        "plt.show()\n",
        "\n",
        "# Print cluster information\n",
        "print(f\"\\nCluster sizes:\")\n",
        "for i in range(hc.n_clusters_):\n",
        "    cluster_size = np.sum(y_hc == i)\n",
        "    print(f\"Cluster {i}: {cluster_size} samples ({cluster_size/len(y_hc)*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What is Agglomerative Clustering?\n",
        "- Agglomerative clustering is a type of hierarchical clustering.\n",
        "    - Start: Treat each data point as its own cluster.\n",
        "    - Merge: Iteratively merge the two closest clusters based on a distance metric.\n",
        "    - Stop: Continue merging until you reach the desired number of clusters or a distance threshold.\n",
        "\n",
        "- Distance between clusters can be calculated in different ways:\n",
        "    - Single linkage: distance between the closest points\n",
        "    - Complete linkage: distance between the farthest points\n",
        "    - Average linkage: average distance between points in clusters\n",
        "    - Ward's method: minimizes variance within clusters (default in scikit-learn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "### Performace Evaluation of Unsupervised Learning Algorithms\n",
        "- No single universal metric for unsupervised evaluation; use a combination of metrics.\n",
        "- Internal metrics (Measures cluster quality based on the data itself) are most commonly used when ground truth is unavailable.\n",
        "- Visualization and domain knowledge are essential for practical validation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Silhouette Score\n",
        "- Measures how well a data point fits within its own cluster compared to other clusters.\n",
        "- Range: -1 to 1, where +1 indicates the point is well matched to its cluster, 0 indicates it lies near a cluster boundary, and -1 indicates possible misassignment to the wrong cluster.\n",
        "- Intuitive and easy to interpret.\n",
        "- Computationally heavy for very large datasets.\n",
        "- Less reliable when clusters have different shapes, densities, or sizes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Silhouette Score vs. Number of Clusters\n",
        "import matplotlib.pyplot as plt\n",
        "# Set environment variables to avoid warnings (if not already set)\n",
        "import os\n",
        "os.environ.setdefault('OMP_NUM_THREADS', '4')\n",
        "os.environ.setdefault('LOKY_MAX_CPU_COUNT', '4')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "warnings.filterwarnings('ignore', message='KMeans is known to have a memory leak')\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Use the same dataset as K-Means\n",
        "# -----------------------------\n",
        "# Note: Make sure to run the K-Means section first to define X_scaled\n",
        "# If X_scaled is not defined, reload and prepare data\n",
        "if 'X_scaled' not in globals():\n",
        "    # Reload data if needed\n",
        "    import pandas as pd\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    df_eval = pd.read_csv('primary_features_boolean_converted_final.csv')\n",
        "    df_eval = df_eval.dropna(subset=['price(Georgian Lari)'])\n",
        "    df_eval['vehicle_age'] = 2024 - df_eval['product_year']\n",
        "    df_eval['luxury_score'] = df_eval[['engine_volume', 'cylinders', 'airbags']].sum(axis=1)\n",
        "    df_eval['safety_score'] = df_eval[['airbags', 'ABS', 'ESP', 'Central Locking', 'Alarm System']].sum(axis=1)\n",
        "    numerical_features_eval = ['vehicle_age', 'luxury_score', 'safety_score', 'mileage', 'engine_volume', 'price(Georgian Lari)']\n",
        "    X_eval = df_eval[numerical_features_eval].fillna(0)\n",
        "    X_sample_eval = X_eval.sample(n=min(1500, len(X_eval)), random_state=42)\n",
        "    scaler_eval = StandardScaler()\n",
        "    X_scaled = scaler_eval.fit_transform(X_sample_eval)\n",
        "\n",
        "print(f\"Evaluating on dataset with shape: {X_scaled.shape}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Test multiple k values\n",
        "# -----------------------------\n",
        "k_values = range(2, 11)  # testing k = 2 to 10\n",
        "silhouette_scores = []\n",
        "\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, init=\"k-means++\", random_state=42)\n",
        "    labels = kmeans.fit_predict(X_scaled)\n",
        "    score = silhouette_score(X_scaled, labels)\n",
        "    silhouette_scores.append(score)\n",
        "    print(f\"k={k}, Average Silhouette Score={score:.3f}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Plot silhouette score vs k\n",
        "# -----------------------------\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(k_values, silhouette_scores, marker=\"o\", linestyle=\"--\")\n",
        "plt.xlabel(\"Number of Clusters (k)\")\n",
        "plt.ylabel(\"Average Silhouette Score\")\n",
        "plt.title(\"Silhouette Score vs Number of Clusters\")\n",
        "plt.xticks(k_values)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Davies–Bouldin Index (DBI)\n",
        "- Measures the average similarity between each cluster and its most similar cluster, considering both cluster scatter and separation.\n",
        "- Lower values indicate better clustering, as clusters are more compact and well-separated.\n",
        "- Computationally efficient and works for various cluster shapes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Davies–Bouldin Index vs. Number of Clusters\n",
        "# Set environment variables to avoid warnings (if not already set)\n",
        "import os\n",
        "os.environ.setdefault('OMP_NUM_THREADS', '4')\n",
        "os.environ.setdefault('LOKY_MAX_CPU_COUNT', '4')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "warnings.filterwarnings('ignore', message='KMeans is known to have a memory leak')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import davies_bouldin_score\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Use the same dataset as K-Means\n",
        "# -----------------------------\n",
        "# Note: Make sure to run the K-Means section first to define X_scaled\n",
        "# If X_scaled is not defined, reload and prepare data\n",
        "if 'X_scaled' not in globals():\n",
        "    # Reload data if needed\n",
        "    import pandas as pd\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    df_eval = pd.read_csv('primary_features_boolean_converted_final.csv')\n",
        "    df_eval = df_eval.dropna(subset=['price(Georgian Lari)'])\n",
        "    df_eval['vehicle_age'] = 2024 - df_eval['product_year']\n",
        "    df_eval['luxury_score'] = df_eval[['engine_volume', 'cylinders', 'airbags']].sum(axis=1)\n",
        "    df_eval['safety_score'] = df_eval[['airbags', 'ABS', 'ESP', 'Central Locking', 'Alarm System']].sum(axis=1)\n",
        "    numerical_features_eval = ['vehicle_age', 'luxury_score', 'safety_score', 'mileage', 'engine_volume', 'price(Georgian Lari)']\n",
        "    X_eval = df_eval[numerical_features_eval].fillna(0)\n",
        "    X_sample_eval = X_eval.sample(n=min(1500, len(X_eval)), random_state=42)\n",
        "    scaler_eval = StandardScaler()\n",
        "    X_scaled = scaler_eval.fit_transform(X_sample_eval)\n",
        "\n",
        "print(f\"Evaluating on dataset with shape: {X_scaled.shape}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Test multiple k values\n",
        "# -----------------------------\n",
        "k_values = range(2, 11)  # testing k = 2 to 10\n",
        "dbi_scores = []\n",
        "\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, init=\"k-means++\", random_state=42)\n",
        "    labels = kmeans.fit_predict(X_scaled)\n",
        "    dbi = davies_bouldin_score(X_scaled, labels)\n",
        "    dbi_scores.append(dbi)\n",
        "    print(f\"k={k}, Davies–Bouldin Index={dbi:.3f}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Plot DBI vs k\n",
        "# -----------------------------\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(k_values, dbi_scores, marker=\"o\", linestyle=\"--\", color=\"green\")\n",
        "plt.xlabel(\"Number of Clusters (k)\")\n",
        "plt.ylabel(\"Davies–Bouldin Index\")\n",
        "plt.title(\"Davies–Bouldin Index vs Number of Clusters\")\n",
        "plt.xticks(k_values)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
